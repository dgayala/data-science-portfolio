# ‚ö° Procesamiento de Big Data con Spark

## üéØ Contexto
Este proyecto corresponde a la evaluaci√≥n final del **M√≥dulo 9 ‚Äì Big Data y Spark**.  
El objetivo fue aplicar t√©cnicas de **procesamiento distribuido y machine learning escalable** con PySpark sobre datasets grandes.

---

## üõ†Ô∏è Metodolog√≠a
1. Carga y procesamiento de datos con RDDs y DataFrames en PySpark.  
2. Aplicaci√≥n de transformaciones (map, flatMap, reduceByKey).  
3. An√°lisis exploratorio con funciones distribuidas.  
4. Entrenamiento de un modelo de clasificaci√≥n usando MLlib.  
5. Evaluaci√≥n del rendimiento del modelo y comparaci√≥n con enfoques tradicionales.  

---

## üìà Resultados principales
- Procesamiento exitoso de un dataset masivo en cl√∫ster simulado.  
- Ejemplos de uso de `VectorAssembler` y modelos de MLlib.  
- Validaci√≥n de que Spark permite escalar algoritmos de ML a vol√∫menes mayores de datos.  

---

## üß∞ Tecnolog√≠as utilizadas
- Python (PySpark, MLlib)  
- Google Colab (con instalaci√≥n de Java y Spark)  

---

## ‚ñ∂Ô∏è C√≥mo ejecutar
1. Clonar este repositorio.  
2. Abrir el notebook `notebook.ipynb` en Google Colab.  
3. Ejecutar la instalaci√≥n de dependencias (Java + PySpark).  
4. Cargar el dataset y correr las celdas en orden.  
